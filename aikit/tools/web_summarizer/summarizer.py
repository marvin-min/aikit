"""
ç½‘é¡µæ€»ç»“å·¥å…· - ä¸»è¦é€»è¾‘
"""
import bs4
from typing import List, Dict, Any

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

from ...core.models import get_client, get_embeddings
from ...core.config import Config
from ...utils.logger import get_logger
from ...utils.helpers import truncate_text

logger = get_logger(__name__)


class WebSummarizer:
    """ç½‘é¡µå†…å®¹æ€»ç»“å·¥å…·"""

    def __init__(self, chunk_size: int = None, chunk_overlap: int = None, retrieval_k: int = None, provider: str = None):
        self.chunk_size = chunk_size or Config.DEFAULT_CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or Config.DEFAULT_CHUNK_OVERLAP
        self.retrieval_k = retrieval_k or Config.DEFAULT_RETRIEVAL_K
        self.provider = provider
        self.logger = logger
    
    def load_documents(self, url: str) -> List[Any]:
        """åŠ è½½ç½‘é¡µæ–‡æ¡£"""
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç† URL: {url}")
        
        try:
            loader = WebBaseLoader(web_path=url)
            docs = loader.load()
            
            if not docs or not docs[0].page_content.strip():
                self.logger.warning("âš ï¸ æ— æ³•è·å–å†…å®¹ï¼Œå°è¯•å…¨é¡µé¢æŠ“å–...")
                loader.bs_kwargs = {}
                docs = loader.load()
            
            return docs
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            raise
    
    def split_documents(self, docs: List[Any]) -> List[Any]:
        """åˆ‡åˆ†æ–‡æ¡£"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        documents = text_splitter.split_documents(docs)
        self.logger.info(f"âœ… æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(documents)} ä¸ªç‰‡æ®µ")
        return documents
    
    def create_vector_store(self, documents: List[Any]):
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=get_embeddings(self.provider)
        )
        return vectorstore
    
    def create_chain(self):
        """åˆ›å»ºRAGé“¾"""
        retriever = self.create_vector_store([]).as_retriever(
            search_kwargs={"k": self.retrieval_k}
        )
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡ˆæ€»ç»“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å†…å®¹è¿›è¡Œæ€»ç»“ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context}"),
            ("user", "ä»»åŠ¡ï¼š{input}\n\næ³¨æ„ï¼šè¯·åˆ†æ¡ç›®åˆ—å‡ºæ ¸å¿ƒè¦ç‚¹,ä¿æŒå®¢è§‚å‡†ç¡®ã€‚"),
        ])
        
        combine_docs_chain = create_stuff_documents_chain(get_client(), prompt)
        rag_chain = create_retrieval_chain(retriever, combine_docs_chain)
        
        return rag_chain
    
    def summarize(self, url: str, query: str = "æ€»ç»“è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒå†…å®¹") -> str:
        """æ€»ç»“ç½‘é¡µå†…å®¹"""
        try:
            # 1. åŠ è½½æ–‡æ¡£
            docs = self.load_documents(url)
            
            # 2. åˆ‡åˆ†æ–‡æ¡£
            documents = self.split_documents(docs)
            
            # 3. åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨
            vectorstore = self.create_vector_store(documents)
            retriever = vectorstore.as_retriever(search_kwargs={"k": self.retrieval_k})
            
            # 4. åˆ›å»ºRAGé“¾
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡ˆæ€»ç»“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å†…å®¹è¿›è¡Œæ€»ç»“ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context}"),
                ("user", "ä»»åŠ¡ï¼š{input}\n\næ³¨æ„ï¼šè¯·åˆ†æ¡ç›®åˆ—å‡ºæ ¸å¿ƒè¦ç‚¹,ä¿æŒå®¢è§‚å‡†ç¡®ã€‚"),
            ])
            
            combine_docs_chain = create_stuff_documents_chain(get_client(provider=self.provider), prompt)
            rag_chain = create_retrieval_chain(retriever, combine_docs_chain)
            
            # 5. è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç‰‡æ®µ
            relevant_docs = retriever.invoke(query)
            self.logger.info("\n[Debug] æ£€ç´¢åˆ°çš„åŸå§‹ç‰‡æ®µé¢„è§ˆ:")
            for i, doc in enumerate(relevant_docs):
                content_snippet = truncate_text(doc.page_content, 60)
                self.logger.info(f"  [{i}] {content_snippet}")
            
            # 6. æ‰§è¡Œæ€»ç»“
            self.logger.info("âœï¸ æ­£åœ¨ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
            result = rag_chain.invoke({"input": query})
            
            return result["answer"]
            
        except Exception as e:
            self.logger.error(f"âŒ æ€»ç»“è¿‡ç¨‹å‡ºé”™: {e}")
            raise